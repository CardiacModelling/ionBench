{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing a new approach in ionBench\n",
    "In this notebook, we will explore how you can take a python implementation of an optimiser and use it in ionBench. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8b19d05e7b0f13"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import ionbench"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:09:01.713437Z",
     "start_time": "2024-11-26T12:08:52.140344Z"
    }
   },
   "id": "6b529376f5d1b957",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the optimiser\n",
    "We will use a simple optimiser that performs a basic gradient descent algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa28484e2d8f2185"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, initial_guess, lr=0.01, tol=1e-6, max_iter=100):\n",
    "    x = copy.deepcopy(initial_guess)  # Initial guess - Think carefully about whether you want to edit initial_guess in place\n",
    "    print(f\"Starting parameters: {x}\")\n",
    "    print(f\"Starting cost: {f(x)}\")\n",
    "    for i in range(max_iter):\n",
    "        grad = df(x)  # Find the local gradient\n",
    "        x -= lr * grad  # Take a step in the direction of the -gradient \n",
    "        print(f\"Iteration {i}\")\n",
    "        print(f\"Parameters: {x}\")\n",
    "        print(f\"Cost: {f(x)}\")\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            # If locally flat, then terminate\n",
    "            break\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:09:01.731652Z",
     "start_time": "2024-11-26T12:09:01.716978Z"
    }
   },
   "id": "a308d38256deed9f",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the optimiser\n",
    "First, we'll check our optimiser is working before we try to use it in ionBench.\n",
    "We will test it against a simple quadratic function, trying to find the minimum at `x=1`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d64239d3235ce5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parameters: 0\n",
      "Starting cost: 1\n",
      "Iteration 0\n",
      "Parameters: 0.02\n",
      "Cost: 0.9603999999999999\n",
      "Iteration 1\n",
      "Parameters: 0.039599999999999996\n",
      "Cost: 0.9223681600000001\n",
      "Iteration 2\n",
      "Parameters: 0.058808\n",
      "Cost: 0.885842380864\n",
      "Iteration 3\n",
      "Parameters: 0.07763184000000001\n",
      "Cost: 0.8507630225817856\n",
      "Iteration 4\n",
      "Parameters: 0.0960792032\n",
      "Cost: 0.8170728068875469\n",
      "Iteration 5\n",
      "Parameters: 0.114157619136\n",
      "Cost: 0.7847167237348001\n",
      "Iteration 6\n",
      "Parameters: 0.13187446675328002\n",
      "Cost: 0.7536419414749018\n",
      "Iteration 7\n",
      "Parameters: 0.14923697741821443\n",
      "Cost: 0.7237977205924958\n",
      "Iteration 8\n",
      "Parameters: 0.16625223786985013\n",
      "Cost: 0.695135330857033\n",
      "Iteration 9\n",
      "Parameters: 0.18292719311245315\n",
      "Cost: 0.6676079717550943\n",
      "Iteration 10\n",
      "Parameters: 0.19926864925020407\n",
      "Cost: 0.6411706960735928\n",
      "Iteration 11\n",
      "Parameters: 0.2152832762652\n",
      "Cost: 0.6157803365090785\n",
      "Iteration 12\n",
      "Parameters: 0.230977610739896\n",
      "Cost: 0.5913954351833189\n",
      "Iteration 13\n",
      "Parameters: 0.2463580585250981\n",
      "Cost: 0.5679761759500596\n",
      "Iteration 14\n",
      "Parameters: 0.2614308973545961\n",
      "Cost: 0.5454843193824371\n",
      "Iteration 15\n",
      "Parameters: 0.2762022794075042\n",
      "Cost: 0.5238831403348926\n",
      "Iteration 16\n",
      "Parameters: 0.2906782338193541\n",
      "Cost: 0.5031373679776309\n",
      "Iteration 17\n",
      "Parameters: 0.304864669142967\n",
      "Cost: 0.48321312820571666\n",
      "Iteration 18\n",
      "Parameters: 0.31876737576010766\n",
      "Cost: 0.4640778883287703\n",
      "Iteration 19\n",
      "Parameters: 0.3323920282449055\n",
      "Cost: 0.445700403950951\n",
      "Iteration 20\n",
      "Parameters: 0.34574418768000736\n",
      "Cost: 0.4280506679544934\n",
      "Iteration 21\n",
      "Parameters: 0.3588293039264072\n",
      "Cost: 0.4110998615034955\n",
      "Iteration 22\n",
      "Parameters: 0.37165271784787907\n",
      "Cost: 0.39482030698795717\n",
      "Iteration 23\n",
      "Parameters: 0.3842196634909215\n",
      "Cost: 0.37918542283123396\n",
      "Iteration 24\n",
      "Parameters: 0.3965352702211031\n",
      "Cost: 0.36416968008711703\n",
      "Iteration 25\n",
      "Parameters: 0.408604564816681\n",
      "Cost: 0.34974856075566735\n",
      "Iteration 26\n",
      "Parameters: 0.4204324735203474\n",
      "Cost: 0.33589851774974283\n",
      "Iteration 27\n",
      "Parameters: 0.43202382404994044\n",
      "Cost: 0.322596936446853\n",
      "Iteration 28\n",
      "Parameters: 0.44338334756894165\n",
      "Cost: 0.30982209776355757\n",
      "Iteration 29\n",
      "Parameters: 0.4545156806175628\n",
      "Cost: 0.2975531426921208\n",
      "Iteration 30\n",
      "Parameters: 0.46542536700521153\n",
      "Cost: 0.28577003824151276\n",
      "Iteration 31\n",
      "Parameters: 0.4761168596651073\n",
      "Cost: 0.2744535447271489\n",
      "Iteration 32\n",
      "Parameters: 0.4865945224718052\n",
      "Cost: 0.2635851843559538\n",
      "Iteration 33\n",
      "Parameters: 0.4968626320223691\n",
      "Cost: 0.25314721105545807\n",
      "Iteration 34\n",
      "Parameters: 0.5069253793819217\n",
      "Cost: 0.24312258149766183\n",
      "Iteration 35\n",
      "Parameters: 0.5167868717942833\n",
      "Cost: 0.23349492727035442\n",
      "Iteration 36\n",
      "Parameters: 0.5264511343583976\n",
      "Cost: 0.22424852815044838\n",
      "Iteration 37\n",
      "Parameters: 0.5359221116712297\n",
      "Cost: 0.2153682864356906\n",
      "Iteration 38\n",
      "Parameters: 0.5452036694378051\n",
      "Cost: 0.20683970229283727\n",
      "Iteration 39\n",
      "Parameters: 0.554299596049049\n",
      "Cost: 0.19864885008204092\n",
      "Iteration 40\n",
      "Parameters: 0.563213604128068\n",
      "Cost: 0.19078235561879212\n",
      "Iteration 41\n",
      "Parameters: 0.5719493320455066\n",
      "Cost: 0.183227374336288\n",
      "Iteration 42\n",
      "Parameters: 0.5805103454045965\n",
      "Cost: 0.17597157031257096\n",
      "Iteration 43\n",
      "Parameters: 0.5889001384965045\n",
      "Cost: 0.1690030961281932\n",
      "Iteration 44\n",
      "Parameters: 0.5971221357265744\n",
      "Cost: 0.1623105735215168\n",
      "Iteration 45\n",
      "Parameters: 0.6051796930120429\n",
      "Cost: 0.1558830748100647\n",
      "Iteration 46\n",
      "Parameters: 0.6130760991518021\n",
      "Cost: 0.1497101050475861\n",
      "Iteration 47\n",
      "Parameters: 0.620814577168766\n",
      "Cost: 0.14378158488770174\n",
      "Iteration 48\n",
      "Parameters: 0.6283982856253907\n",
      "Cost: 0.13808783412614872\n",
      "Iteration 49\n",
      "Parameters: 0.6358303199128829\n",
      "Cost: 0.13261955589475324\n",
      "Iteration 50\n",
      "Parameters: 0.6431137135146252\n",
      "Cost: 0.127367821481321\n",
      "Iteration 51\n",
      "Parameters: 0.6502514392443327\n",
      "Cost: 0.1223240557506607\n",
      "Iteration 52\n",
      "Parameters: 0.6572464104594461\n",
      "Cost: 0.1174800231429345\n",
      "Iteration 53\n",
      "Parameters: 0.6641014822502572\n",
      "Cost: 0.11282781422647427\n",
      "Iteration 54\n",
      "Parameters: 0.6708194526052521\n",
      "Cost: 0.10835983278310586\n",
      "Iteration 55\n",
      "Parameters: 0.6774030635531471\n",
      "Cost: 0.10406878340489488\n",
      "Iteration 56\n",
      "Parameters: 0.6838550022820841\n",
      "Cost: 0.09994765958206107\n",
      "Iteration 57\n",
      "Parameters: 0.6901779022364424\n",
      "Cost: 0.09598973226261145\n",
      "Iteration 58\n",
      "Parameters: 0.6963743441917135\n",
      "Cost: 0.09218853886501205\n",
      "Iteration 59\n",
      "Parameters: 0.7024468573078793\n",
      "Cost: 0.08853787272595756\n",
      "Iteration 60\n",
      "Parameters: 0.7083979201617217\n",
      "Cost: 0.08503177296600967\n",
      "Iteration 61\n",
      "Parameters: 0.7142299617584872\n",
      "Cost: 0.0816645147565557\n",
      "Iteration 62\n",
      "Parameters: 0.7199453625233174\n",
      "Cost: 0.0784305999721961\n",
      "Iteration 63\n",
      "Parameters: 0.7255464552728511\n",
      "Cost: 0.07532474821329711\n",
      "Iteration 64\n",
      "Parameters: 0.7310355261673941\n",
      "Cost: 0.07234188818405055\n",
      "Iteration 65\n",
      "Parameters: 0.7364148156440462\n",
      "Cost: 0.06947714941196215\n",
      "Iteration 66\n",
      "Parameters: 0.7416865193311653\n",
      "Cost: 0.06672585429524847\n",
      "Iteration 67\n",
      "Parameters: 0.746852788944542\n",
      "Cost: 0.0640835104651566\n",
      "Iteration 68\n",
      "Parameters: 0.7519157331656512\n",
      "Cost: 0.06154580345073638\n",
      "Iteration 69\n",
      "Parameters: 0.7568774185023381\n",
      "Cost: 0.05910858963408723\n",
      "Iteration 70\n",
      "Parameters: 0.7617398701322914\n",
      "Cost: 0.05676788948457737\n",
      "Iteration 71\n",
      "Parameters: 0.7665050727296455\n",
      "Cost: 0.05451988106098813\n",
      "Iteration 72\n",
      "Parameters: 0.7711749712750526\n",
      "Cost: 0.052360893770972985\n",
      "Iteration 73\n",
      "Parameters: 0.7757514718495516\n",
      "Cost: 0.05028740237764244\n",
      "Iteration 74\n",
      "Parameters: 0.7802364424125606\n",
      "Cost: 0.04829602124348778\n",
      "Iteration 75\n",
      "Parameters: 0.7846317135643094\n",
      "Cost: 0.046383498802245675\n",
      "Iteration 76\n",
      "Parameters: 0.7889390792930232\n",
      "Cost: 0.04454671224967675\n",
      "Iteration 77\n",
      "Parameters: 0.7931602977071628\n",
      "Cost: 0.04278266244458953\n",
      "Iteration 78\n",
      "Parameters: 0.7972970917530195\n",
      "Cost: 0.04108846901178378\n",
      "Iteration 79\n",
      "Parameters: 0.8013511499179592\n",
      "Cost: 0.039461365638917126\n",
      "Iteration 80\n",
      "Parameters: 0.8053241269196\n",
      "Cost: 0.037898695559616016\n",
      "Iteration 81\n",
      "Parameters: 0.809217644381208\n",
      "Cost: 0.03639790721545521\n",
      "Iteration 82\n",
      "Parameters: 0.8130332914935838\n",
      "Cost: 0.03495655008972319\n",
      "Iteration 83\n",
      "Parameters: 0.8167726256637121\n",
      "Cost: 0.033572270706170165\n",
      "Iteration 84\n",
      "Parameters: 0.8204371731504378\n",
      "Cost: 0.03224280878620584\n",
      "Iteration 85\n",
      "Parameters: 0.8240284296874291\n",
      "Cost: 0.030965993558272096\n",
      "Iteration 86\n",
      "Parameters: 0.8275478610936805\n",
      "Cost: 0.029739740213364514\n",
      "Iteration 87\n",
      "Parameters: 0.8309969038718069\n",
      "Cost: 0.02856204650091527\n",
      "Iteration 88\n",
      "Parameters: 0.8343769657943708\n",
      "Cost: 0.027430989459479013\n",
      "Iteration 89\n",
      "Parameters: 0.8376894264784834\n",
      "Cost: 0.026344722276883635\n",
      "Iteration 90\n",
      "Parameters: 0.8409356379489138\n",
      "Cost: 0.025301471274719035\n",
      "Iteration 91\n",
      "Parameters: 0.8441169251899355\n",
      "Cost: 0.024299533012240155\n",
      "Iteration 92\n",
      "Parameters: 0.8472345866861368\n",
      "Cost: 0.023337271504955445\n",
      "Iteration 93\n",
      "Parameters: 0.8502898949524141\n",
      "Cost: 0.022413115553359193\n",
      "Iteration 94\n",
      "Parameters: 0.8532840970533658\n",
      "Cost: 0.021525556177446177\n",
      "Iteration 95\n",
      "Parameters: 0.8562184151122985\n",
      "Cost: 0.020673144152819324\n",
      "Iteration 96\n",
      "Parameters: 0.8590940468100525\n",
      "Cost: 0.019854487644367667\n",
      "Iteration 97\n",
      "Parameters: 0.8619121658738514\n",
      "Cost: 0.019068249933650718\n",
      "Iteration 98\n",
      "Parameters: 0.8646739225563744\n",
      "Cost: 0.018313147236278145\n",
      "Iteration 99\n",
      "Parameters: 0.867380444105247\n",
      "Cost: 0.01758794660572153\n",
      "Optimiser parameter is 0.867380444105247\n"
     ]
    }
   ],
   "source": [
    "x0 = 0\n",
    "f = lambda x: (x - 1)**2\n",
    "df = lambda x: 2 * (x - 1)\n",
    "x_opt = gradient_descent(f, df, x0)\n",
    "print(f\"Optimiser parameter is {x_opt}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:09:01.765901Z",
     "start_time": "2024-11-26T12:09:01.735080Z"
    }
   },
   "id": "1e1497cd4efa563a",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the optimiser in ionBench\n",
    "Now that we have seen the optimiser working, we will look at how this code needs to change to work with ionBench. \n",
    "\n",
    "Firstly, we need to change the function signature to take an ionBench Benchmarker object, rather than the function and its gradient.\n",
    "\n",
    "Next, we need to change any cost and gradient calls to use this new Benchmarker object.\n",
    "\n",
    "Finally, we should rename `initial_guess` to `x0` to match the ionBench API (this is used by `ionBench.multistart`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acc80a68b598b5e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ionbench_gradient_descent(bm, x0, lr=1e-4, tol=1e-6, max_iter=100):\n",
    "    x = copy.deepcopy(x0)  # Initial guess\n",
    "    print(f\"Starting parameters: {x}\")\n",
    "    print(f\"Starting cost: {bm.cost(x)}\")\n",
    "    for i in range(max_iter):\n",
    "        grad = bm.grad(x)  # Find the local gradient\n",
    "        x -= lr * grad  # Take a step in the direction of the -gradient \n",
    "        print(f\"Iteration {i}\")\n",
    "        print(f\"Parameters: {x}\")\n",
    "        print(f\"Cost: {bm.cost(x)}\")\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            # If locally flat, then terminate\n",
    "            break\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:09:01.780695Z",
     "start_time": "2024-11-26T12:09:01.770566Z"
    }
   },
   "id": "25281828d4de077c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing the optimiser in ionBench\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b5b8d3f7d212dd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Loewe 2016 IKr benchmark\n",
      "Benchmarker initialised\n",
      "Starting parameters: [ 3.96983773e-05  1.79937089e+01  7.21619706e+00 -3.36333427e+01\n",
      "  5.72783381e-01  2.45777886e-01  1.56151240e+01  3.82248546e+00\n",
      "  5.50466596e+01  3.37774528e+00  2.66266373e-01  6.33325210e+02]\n",
      "Starting cost: 0.5485281798573118\n",
      "Iteration 0\n",
      "Parameters: [-7.58040982e-01  1.79937082e+01  7.21619691e+00 -3.36333424e+01\n",
      "  5.72783381e-01  2.45764600e-01  1.56151240e+01  3.82248553e+00\n",
      "  5.50466596e+01  3.37774529e+00  2.66124510e-01  6.33325210e+02]\n",
      "Cost: 1.6640006326750434\n",
      "Iteration 1\n",
      "Parameters: [-7.58039748e-01  1.79937081e+01  7.21619486e+00 -3.36333418e+01\n",
      "  5.72783381e-01  2.45943437e-01  1.56151229e+01  3.82248674e+00\n",
      "  5.50466596e+01  3.37774534e+00  2.65545051e-01  6.33325210e+02]\n",
      "Cost: 1.6603238379687733\n",
      "Iteration 2\n",
      "Parameters: [-7.58038517e-01  1.79937080e+01  7.21619282e+00 -3.36333412e+01\n",
      "  5.72783381e-01  2.46121794e-01  1.56151218e+01  3.82248794e+00\n",
      "  5.50466596e+01  3.37774538e+00  2.64965737e-01  6.33325209e+02]\n",
      "Cost: 1.6566504412754468\n",
      "Iteration 3\n",
      "Parameters: [-7.58037287e-01  1.79937079e+01  7.21619078e+00 -3.36333406e+01\n",
      "  5.72783381e-01  2.46299670e-01  1.56151207e+01  3.82248914e+00\n",
      "  5.50466596e+01  3.37774543e+00  2.64386568e-01  6.33325209e+02]\n",
      "Cost: 1.6529804326280204\n",
      "Iteration 4\n",
      "Parameters: [-7.58036060e-01  1.79937077e+01  7.21618875e+00 -3.36333400e+01\n",
      "  5.72783381e-01  2.46477068e-01  1.56151196e+01  3.82249034e+00\n",
      "  5.50466596e+01  3.37774547e+00  2.63807544e-01  6.33325208e+02]\n",
      "Cost: 1.649313800839287\n",
      "Iteration 5\n",
      "Parameters: [-7.58034835e-01  1.79937076e+01  7.21618672e+00 -3.36333395e+01\n",
      "  5.72783381e-01  2.46653988e-01  1.56151185e+01  3.82249153e+00\n",
      "  5.50466596e+01  3.37774552e+00  2.63228664e-01  6.33325208e+02]\n",
      "Cost: 1.6456505357031523\n",
      "Iteration 6\n",
      "Parameters: [-7.58033612e-01  1.79937075e+01  7.21618469e+00 -3.36333389e+01\n",
      "  5.72783381e-01  2.46830430e-01  1.56151174e+01  3.82249272e+00\n",
      "  5.50466596e+01  3.37774556e+00  2.62649929e-01  6.33325208e+02]\n",
      "Cost: 1.6419906271701068\n",
      "Iteration 7\n",
      "Parameters: [-7.58032392e-01  1.79937074e+01  7.21618266e+00 -3.36333383e+01\n",
      "  5.72783381e-01  2.47006396e-01  1.56151163e+01  3.82249391e+00\n",
      "  5.50466596e+01  3.37774560e+00  2.62071338e-01  6.33325207e+02]\n",
      "Cost: 1.63833406492967\n",
      "Iteration 8\n",
      "Parameters: [-7.58031174e-01  1.79937073e+01  7.21618064e+00 -3.36333377e+01\n",
      "  5.72783381e-01  2.47181885e-01  1.56151152e+01  3.82249509e+00\n",
      "  5.50466596e+01  3.37774565e+00  2.61492891e-01  6.33325207e+02]\n",
      "Cost: 1.634680838306521\n",
      "Iteration 9\n",
      "Parameters: [-7.58029957e-01  1.79937071e+01  7.21617863e+00 -3.36333371e+01\n",
      "  5.72783381e-01  2.47356898e-01  1.56151141e+01  3.82249627e+00\n",
      "  5.50466596e+01  3.37774569e+00  2.60914587e-01  6.33325206e+02]\n",
      "Cost: 1.6310309383825365\n",
      "Iteration 10\n",
      "Parameters: [-7.58028743e-01  1.79937070e+01  7.21617662e+00 -3.36333366e+01\n",
      "  5.72783381e-01  2.47531436e-01  1.56151131e+01  3.82249745e+00\n",
      "  5.50466596e+01  3.37774574e+00  2.60336427e-01  6.33325206e+02]\n",
      "Cost: 1.6273843544997748\n",
      "Iteration 11\n",
      "Parameters: [-7.58027532e-01  1.79937069e+01  7.21617461e+00 -3.36333360e+01\n",
      "  5.72783381e-01  2.47705499e-01  1.56151120e+01  3.82249863e+00\n",
      "  5.50466596e+01  3.37774578e+00  2.59758410e-01  6.33325206e+02]\n",
      "Cost: 1.6237410762685112\n",
      "Iteration 12\n",
      "Parameters: [-7.58026322e-01  1.79937068e+01  7.21617260e+00 -3.36333354e+01\n",
      "  5.72783381e-01  2.47879089e-01  1.56151109e+01  3.82249980e+00\n",
      "  5.50466596e+01  3.37774582e+00  2.59180535e-01  6.33325205e+02]\n",
      "Cost: 1.620101094155789\n",
      "Iteration 13\n",
      "Parameters: [-7.58025115e-01  1.79937066e+01  7.21617060e+00 -3.36333348e+01\n",
      "  5.72783381e-01  2.48052205e-01  1.56151098e+01  3.82250097e+00\n",
      "  5.50466596e+01  3.37774587e+00  2.58602803e-01  6.33325205e+02]\n",
      "Cost: 1.616464398048059\n",
      "Iteration 14\n",
      "Parameters: [-7.58023910e-01  1.79937065e+01  7.21616861e+00 -3.36333343e+01\n",
      "  5.72783381e-01  2.48224849e-01  1.56151088e+01  3.82250214e+00\n",
      "  5.50466596e+01  3.37774591e+00  2.58025214e-01  6.33325205e+02]\n",
      "Cost: 1.612830977709769\n",
      "Iteration 15\n",
      "Parameters: [-7.58022707e-01  1.79937064e+01  7.21616661e+00 -3.36333337e+01\n",
      "  5.72783381e-01  2.48397021e-01  1.56151077e+01  3.82250330e+00\n",
      "  5.50466597e+01  3.37774596e+00  2.57447766e-01  6.33325204e+02]\n",
      "Cost: 1.6092008236096997\n",
      "Iteration 16\n",
      "Parameters: [-7.58021506e-01  1.79937063e+01  7.21616462e+00 -3.36333331e+01\n",
      "  5.72783381e-01  2.48568721e-01  1.56151067e+01  3.82250446e+00\n",
      "  5.50466597e+01  3.37774600e+00  2.56870460e-01  6.33325204e+02]\n",
      "Cost: 1.6055739260005413\n",
      "Iteration 17\n",
      "Parameters: [-7.58020308e-01  1.79937062e+01  7.21616264e+00 -3.36333326e+01\n",
      "  5.72783381e-01  2.48739950e-01  1.56151056e+01  3.82250562e+00\n",
      "  5.50466597e+01  3.37774604e+00  2.56293296e-01  6.33325203e+02]\n",
      "Cost: 1.6019502750828805\n",
      "Iteration 18\n",
      "Parameters: [-7.58019112e-01  1.79937061e+01  7.21616066e+00 -3.36333320e+01\n",
      "  5.72783381e-01  2.48910710e-01  1.56151045e+01  3.82250678e+00\n",
      "  5.50466597e+01  3.37774608e+00  2.55716273e-01  6.33325203e+02]\n",
      "Cost: 1.5983298612159478\n",
      "Iteration 19\n",
      "Parameters: [-7.58017918e-01  1.79937059e+01  7.21615868e+00 -3.36333314e+01\n",
      "  5.72783381e-01  2.49080999e-01  1.56151035e+01  3.82250793e+00\n",
      "  5.50466597e+01  3.37774613e+00  2.55139391e-01  6.33325203e+02]\n",
      "Cost: 1.5947126746203732\n",
      "Iteration 20\n",
      "Parameters: [-7.58016726e-01  1.79937058e+01  7.21615671e+00 -3.36333309e+01\n",
      "  5.72783381e-01  2.49250820e-01  1.56151024e+01  3.82250908e+00\n",
      "  5.50466597e+01  3.37774617e+00  2.54562650e-01  6.33325202e+02]\n",
      "Cost: 1.5910987060366215\n",
      "Iteration 21\n",
      "Parameters: [-7.58015537e-01  1.79937057e+01  7.21615474e+00 -3.36333303e+01\n",
      "  5.72783381e-01  2.49420172e-01  1.56151014e+01  3.82251022e+00\n",
      "  5.50466597e+01  3.37774621e+00  2.53986050e-01  6.33325202e+02]\n",
      "Cost: 1.5874879453190072\n",
      "Iteration 22\n",
      "Parameters: [-7.58014350e-01  1.79937056e+01  7.21615277e+00 -3.36333298e+01\n",
      "  5.72783381e-01  2.49589057e-01  1.56151003e+01  3.82251137e+00\n",
      "  5.50466597e+01  3.37774625e+00  2.53409590e-01  6.33325201e+02]\n",
      "Cost: 1.5838803833052726\n",
      "Iteration 23\n",
      "Parameters: [-7.58013165e-01  1.79937055e+01  7.21615081e+00 -3.36333292e+01\n",
      "  5.72783381e-01  2.49757474e-01  1.56150993e+01  3.82251251e+00\n",
      "  5.50466597e+01  3.37774630e+00  2.52833270e-01  6.33325201e+02]\n",
      "Cost: 1.5802760104542257\n",
      "Iteration 24\n",
      "Parameters: [-7.58011982e-01  1.79937053e+01  7.21614885e+00 -3.36333286e+01\n",
      "  5.72783381e-01  2.49925425e-01  1.56150983e+01  3.82251364e+00\n",
      "  5.50466597e+01  3.37774634e+00  2.52257090e-01  6.33325201e+02]\n",
      "Cost: 1.5766748174542127\n",
      "Iteration 25\n",
      "Parameters: [-7.58010802e-01  1.79937052e+01  7.21614690e+00 -3.36333281e+01\n",
      "  5.72783381e-01  2.50092909e-01  1.56150972e+01  3.82251478e+00\n",
      "  5.50466597e+01  3.37774638e+00  2.51681050e-01  6.33325200e+02]\n",
      "Cost: 1.5730767948723308\n",
      "Iteration 26\n",
      "Parameters: [-7.58009624e-01  1.79937051e+01  7.21614495e+00 -3.36333275e+01\n",
      "  5.72783381e-01  2.50259928e-01  1.56150962e+01  3.82251591e+00\n",
      "  5.50466597e+01  3.37774642e+00  2.51105149e-01  6.33325200e+02]\n",
      "Cost: 1.569481933400558\n",
      "Iteration 27\n",
      "Parameters: [-7.58008448e-01  1.79937050e+01  7.21614300e+00 -3.36333270e+01\n",
      "  5.72783381e-01  2.50426483e-01  1.56150952e+01  3.82251704e+00\n",
      "  5.50466597e+01  3.37774647e+00  2.50529387e-01  6.33325200e+02]\n",
      "Cost: 1.5658902237647785\n",
      "Iteration 28\n",
      "Parameters: [-7.58007274e-01  1.79937049e+01  7.21614106e+00 -3.36333264e+01\n",
      "  5.72783381e-01  2.50592573e-01  1.56150941e+01  3.82251816e+00\n",
      "  5.50466597e+01  3.37774651e+00  2.49953765e-01  6.33325199e+02]\n",
      "Cost: 1.5623016567360566\n",
      "Iteration 29\n",
      "Parameters: [-7.58006103e-01  1.79937048e+01  7.21613912e+00 -3.36333259e+01\n",
      "  5.72783381e-01  2.50758199e-01  1.56150931e+01  3.82251929e+00\n",
      "  5.50466597e+01  3.37774655e+00  2.49378281e-01  6.33325199e+02]\n",
      "Cost: 1.558716223163981\n",
      "Iteration 30\n",
      "Parameters: [-7.58004934e-01  1.79937046e+01  7.21613719e+00 -3.36333253e+01\n",
      "  5.72783381e-01  2.50923363e-01  1.56150921e+01  3.82252041e+00\n",
      "  5.50466597e+01  3.37774659e+00  2.48802935e-01  6.33325198e+02]\n",
      "Cost: 1.5551339135900613\n",
      "Iteration 31\n",
      "Parameters: [-7.58003768e-01  1.79937045e+01  7.21613526e+00 -3.36333248e+01\n",
      "  5.72783381e-01  2.51088064e-01  1.56150911e+01  3.82252152e+00\n",
      "  5.50466597e+01  3.37774663e+00  2.48227728e-01  6.33325198e+02]\n",
      "Cost: 1.5515547196314037\n",
      "Iteration 32\n",
      "Parameters: [-7.58002603e-01  1.79937044e+01  7.21613333e+00 -3.36333242e+01\n",
      "  5.72783381e-01  2.51252303e-01  1.56150900e+01  3.82252264e+00\n",
      "  5.50466597e+01  3.37774667e+00  2.47652659e-01  6.33325198e+02]\n",
      "Cost: 1.547978631780676\n",
      "Iteration 33\n",
      "Parameters: [-7.58001441e-01  1.79937043e+01  7.21613141e+00 -3.36333237e+01\n",
      "  5.72783381e-01  2.51416080e-01  1.56150890e+01  3.82252375e+00\n",
      "  5.50466598e+01  3.37774671e+00  2.47077728e-01  6.33325197e+02]\n",
      "Cost: 1.544405641099634\n",
      "Iteration 34\n",
      "Parameters: [-7.58000281e-01  1.79937042e+01  7.21612949e+00 -3.36333231e+01\n",
      "  5.72783381e-01  2.51579397e-01  1.56150880e+01  3.82252486e+00\n",
      "  5.50466598e+01  3.37774675e+00  2.46502935e-01  6.33325197e+02]\n",
      "Cost: 1.5408357384975722\n",
      "Iteration 35\n",
      "Parameters: [-7.57999124e-01  1.79937041e+01  7.21612758e+00 -3.36333226e+01\n",
      "  5.72783381e-01  2.51742254e-01  1.56150870e+01  3.82252596e+00\n",
      "  5.50466598e+01  3.37774679e+00  2.45928279e-01  6.33325197e+02]\n",
      "Cost: 1.5372689154214385\n",
      "Iteration 36\n",
      "Parameters: [-7.57997969e-01  1.79937039e+01  7.21612567e+00 -3.36333220e+01\n",
      "  5.72783381e-01  2.51904651e-01  1.56150860e+01  3.82252707e+00\n",
      "  5.50466598e+01  3.37774684e+00  2.45353760e-01  6.33325196e+02]\n",
      "Cost: 1.533705162837342\n",
      "Iteration 37\n",
      "Parameters: [-7.57996816e-01  1.79937038e+01  7.21612376e+00 -3.36333215e+01\n",
      "  5.72783381e-01  2.52066588e-01  1.56150850e+01  3.82252817e+00\n",
      "  5.50466598e+01  3.37774688e+00  2.44779379e-01  6.33325196e+02]\n",
      "Cost: 1.5301444731924836\n",
      "Iteration 38\n",
      "Parameters: [-7.57995665e-01  1.79937037e+01  7.21612186e+00 -3.36333209e+01\n",
      "  5.72783381e-01  2.52228067e-01  1.56150840e+01  3.82252926e+00\n",
      "  5.50466598e+01  3.37774692e+00  2.44205134e-01  6.33325195e+02]\n",
      "Cost: 1.526586835278547\n",
      "Iteration 39\n",
      "Parameters: [-7.57994517e-01  1.79937036e+01  7.21611996e+00 -3.36333204e+01\n",
      "  5.72783381e-01  2.52389089e-01  1.56150830e+01  3.82253036e+00\n",
      "  5.50466598e+01  3.37774696e+00  2.43631026e-01  6.33325195e+02]\n",
      "Cost: 1.5230322415829678\n",
      "Iteration 40\n",
      "Parameters: [-7.57993371e-01  1.79937035e+01  7.21611806e+00 -3.36333199e+01\n",
      "  5.72783381e-01  2.52549653e-01  1.56150820e+01  3.82253145e+00\n",
      "  5.50466598e+01  3.37774700e+00  2.43057054e-01  6.33325195e+02]\n",
      "Cost: 1.5194806834615997\n",
      "Iteration 41\n",
      "Parameters: [-7.57992227e-01  1.79937034e+01  7.21611617e+00 -3.36333193e+01\n",
      "  5.72783381e-01  2.52709760e-01  1.56150810e+01  3.82253254e+00\n",
      "  5.50466598e+01  3.37774704e+00  2.42483219e-01  6.33325194e+02]\n",
      "Cost: 1.515932152327296\n",
      "Iteration 42\n",
      "Parameters: [-7.57991086e-01  1.79937033e+01  7.21611429e+00 -3.36333188e+01\n",
      "  5.72783381e-01  2.52869411e-01  1.56150800e+01  3.82253362e+00\n",
      "  5.50466598e+01  3.37774708e+00  2.41909520e-01  6.33325194e+02]\n",
      "Cost: 1.5123866394022836\n",
      "Iteration 43\n",
      "Parameters: [-7.57989947e-01  1.79937031e+01  7.21611240e+00 -3.36333182e+01\n",
      "  5.72783381e-01  2.53028606e-01  1.56150790e+01  3.82253470e+00\n",
      "  5.50466598e+01  3.37774712e+00  2.41335956e-01  6.33325194e+02]\n",
      "Cost: 1.508844136369901\n",
      "Iteration 44\n",
      "Parameters: [-7.57988811e-01  1.79937030e+01  7.21611053e+00 -3.36333177e+01\n",
      "  5.72783381e-01  2.53187346e-01  1.56150780e+01  3.82253578e+00\n",
      "  5.50466598e+01  3.37774716e+00  2.40762528e-01  6.33325193e+02]\n",
      "Cost: 1.5053046348856334\n",
      "Iteration 45\n",
      "Parameters: [-7.57987676e-01  1.79937029e+01  7.21610865e+00 -3.36333172e+01\n",
      "  5.72783381e-01  2.53345631e-01  1.56150770e+01  3.82253686e+00\n",
      "  5.50466598e+01  3.37774720e+00  2.40189236e-01  6.33325193e+02]\n",
      "Cost: 1.5017681263765286\n",
      "Iteration 46\n",
      "Parameters: [-7.57986544e-01  1.79937028e+01  7.21610678e+00 -3.36333166e+01\n",
      "  5.72783381e-01  2.53503460e-01  1.56150760e+01  3.82253793e+00\n",
      "  5.50466598e+01  3.37774724e+00  2.39616079e-01  6.33325192e+02]\n",
      "Cost: 1.4982346059183047\n",
      "Iteration 47\n",
      "Parameters: [-7.57985414e-01  1.79937027e+01  7.21610491e+00 -3.36333161e+01\n",
      "  5.72783381e-01  2.53660838e-01  1.56150751e+01  3.82253900e+00\n",
      "  5.50466598e+01  3.37774727e+00  2.39043057e-01  6.33325192e+02]\n",
      "Cost: 1.4947040582007016\n",
      "Iteration 48\n",
      "Parameters: [-7.57984287e-01  1.79937026e+01  7.21610305e+00 -3.36333156e+01\n",
      "  5.72783381e-01  2.53817762e-01  1.56150741e+01  3.82254007e+00\n",
      "  5.50466598e+01  3.37774731e+00  2.38470170e-01  6.33325192e+02]\n",
      "Cost: 1.4911764786089643\n",
      "Iteration 49\n",
      "Parameters: [-7.57983162e-01  1.79937025e+01  7.21610119e+00 -3.36333150e+01\n",
      "  5.72783381e-01  2.53974235e-01  1.56150731e+01  3.82254114e+00\n",
      "  5.50466598e+01  3.37774735e+00  2.37897417e-01  6.33325191e+02]\n",
      "Cost: 1.487651858553177\n",
      "Iteration 50\n",
      "Parameters: [-7.57982040e-01  1.79937024e+01  7.21609934e+00 -3.36333145e+01\n",
      "  5.72783381e-01  2.54130255e-01  1.56150721e+01  3.82254220e+00\n",
      "  5.50466598e+01  3.37774739e+00  2.37324800e-01  6.33325191e+02]\n",
      "Cost: 1.4841301900081254\n",
      "Iteration 51\n",
      "Parameters: [-7.57980919e-01  1.79937022e+01  7.21609749e+00 -3.36333140e+01\n",
      "  5.72783381e-01  2.54285824e-01  1.56150712e+01  3.82254326e+00\n",
      "  5.50466598e+01  3.37774743e+00  2.36752316e-01  6.33325191e+02]\n",
      "Cost: 1.4806114648228044\n",
      "Iteration 52\n",
      "Parameters: [-7.57979802e-01  1.79937021e+01  7.21609564e+00 -3.36333135e+01\n",
      "  5.72783381e-01  2.54440942e-01  1.56150702e+01  3.82254431e+00\n",
      "  5.50466599e+01  3.37774747e+00  2.36179967e-01  6.33325190e+02]\n",
      "Cost: 1.4770956748953057\n",
      "Iteration 53\n",
      "Parameters: [-7.57978686e-01  1.79937020e+01  7.21609380e+00 -3.36333129e+01\n",
      "  5.72783381e-01  2.54595610e-01  1.56150692e+01  3.82254537e+00\n",
      "  5.50466599e+01  3.37774751e+00  2.35607752e-01  6.33325190e+02]\n",
      "Cost: 1.4735828121683492\n",
      "Iteration 54\n",
      "Parameters: [-7.57977573e-01  1.79937019e+01  7.21609196e+00 -3.36333124e+01\n",
      "  5.72783381e-01  2.54749828e-01  1.56150683e+01  3.82254642e+00\n",
      "  5.50466599e+01  3.37774755e+00  2.35035670e-01  6.33325190e+02]\n",
      "Cost: 1.4700728686442783\n",
      "Iteration 55\n",
      "Parameters: [-7.57976462e-01  1.79937018e+01  7.21609013e+00 -3.36333119e+01\n",
      "  5.72783381e-01  2.54903597e-01  1.56150673e+01  3.82254747e+00\n",
      "  5.50466599e+01  3.37774759e+00  2.34463723e-01  6.33325189e+02]\n",
      "Cost: 1.466565836519214\n",
      "Iteration 56\n",
      "Parameters: [-7.57975354e-01  1.79937017e+01  7.21608830e+00 -3.36333114e+01\n",
      "  5.72783381e-01  2.55056917e-01  1.56150664e+01  3.82254851e+00\n",
      "  5.50466599e+01  3.37774762e+00  2.33891909e-01  6.33325189e+02]\n",
      "Cost: 1.4630617074995005\n",
      "Iteration 57\n",
      "Parameters: [-7.57974248e-01  1.79937016e+01  7.21608647e+00 -3.36333109e+01\n",
      "  5.72783381e-01  2.55209789e-01  1.56150654e+01  3.82254955e+00\n",
      "  5.50466599e+01  3.37774766e+00  2.33320228e-01  6.33325189e+02]\n",
      "Cost: 1.4595604739091887\n",
      "Iteration 58\n",
      "Parameters: [-7.57973144e-01  1.79937015e+01  7.21608465e+00 -3.36333103e+01\n",
      "  5.72783381e-01  2.55362213e-01  1.56150645e+01  3.82255059e+00\n",
      "  5.50466599e+01  3.37774770e+00  2.32748680e-01  6.33325188e+02]\n",
      "Cost: 1.4560621279237205\n",
      "Iteration 59\n",
      "Parameters: [-7.57972043e-01  1.79937014e+01  7.21608283e+00 -3.36333098e+01\n",
      "  5.72783381e-01  2.55514191e-01  1.56150635e+01  3.82255163e+00\n",
      "  5.50466599e+01  3.37774774e+00  2.32177266e-01  6.33325188e+02]\n",
      "Cost: 1.4525666617324993\n",
      "Iteration 60\n",
      "Parameters: [-7.57970944e-01  1.79937013e+01  7.21608102e+00 -3.36333093e+01\n",
      "  5.72783381e-01  2.55665721e-01  1.56150626e+01  3.82255266e+00\n",
      "  5.50466599e+01  3.37774778e+00  2.31605984e-01  6.33325188e+02]\n",
      "Cost: 1.4490740676615248\n",
      "Iteration 61\n",
      "Parameters: [-7.57969848e-01  1.79937011e+01  7.21607921e+00 -3.36333088e+01\n",
      "  5.72783381e-01  2.55816806e-01  1.56150616e+01  3.82255369e+00\n",
      "  5.50466599e+01  3.37774781e+00  2.31034835e-01  6.33325187e+02]\n",
      "Cost: 1.4455843380816327\n",
      "Iteration 62\n",
      "Parameters: [-7.57968754e-01  1.79937010e+01  7.21607741e+00 -3.36333083e+01\n",
      "  5.72783381e-01  2.55967445e-01  1.56150607e+01  3.82255472e+00\n",
      "  5.50466599e+01  3.37774785e+00  2.30463818e-01  6.33325187e+02]\n",
      "Cost: 1.442097465138239\n",
      "Iteration 63\n",
      "Parameters: [-7.57967662e-01  1.79937009e+01  7.21607560e+00 -3.36333078e+01\n",
      "  5.72783381e-01  2.56117638e-01  1.56150597e+01  3.82255575e+00\n",
      "  5.50466599e+01  3.37774789e+00  2.29892934e-01  6.33325187e+02]\n",
      "Cost: 1.4386134416417282\n",
      "Iteration 64\n",
      "Parameters: [-7.57966573e-01  1.79937008e+01  7.21607381e+00 -3.36333073e+01\n",
      "  5.72783381e-01  2.56267388e-01  1.56150588e+01  3.82255677e+00\n",
      "  5.50466599e+01  3.37774793e+00  2.29322182e-01  6.33325186e+02]\n",
      "Cost: 1.4351322595302616\n",
      "Iteration 65\n",
      "Parameters: [-7.57965482e-01  1.79937007e+01  7.21607201e+00 -3.36333067e+01\n",
      "  5.72783381e-01  2.56416683e-01  1.56150579e+01  3.82255779e+00\n",
      "  5.50466599e+01  3.37774796e+00  2.28751563e-01  6.33325186e+02]\n",
      "Cost: 1.431653927055106\n",
      "Iteration 66\n",
      "Parameters: [-7.57964398e-01  1.79937006e+01  7.21607022e+00 -3.36333062e+01\n",
      "  5.72783381e-01  2.56565544e-01  1.56150570e+01  3.82255880e+00\n",
      "  5.50466599e+01  3.37774800e+00  2.28181075e-01  6.33325185e+02]\n",
      "Cost: 1.4281784059892828\n",
      "Iteration 67\n",
      "Parameters: [-7.57963316e-01  1.79937005e+01  7.21606844e+00 -3.36333057e+01\n",
      "  5.72783381e-01  2.56713963e-01  1.56150560e+01  3.82255982e+00\n",
      "  5.50466599e+01  3.37774804e+00  2.27610719e-01  6.33325185e+02]\n",
      "Cost: 1.4247057039077085\n",
      "Iteration 68\n",
      "Parameters: [-7.57962234e-01  1.79937004e+01  7.21606666e+00 -3.36333052e+01\n",
      "  5.72783381e-01  2.56861934e-01  1.56150551e+01  3.82256083e+00\n",
      "  5.50466599e+01  3.37774807e+00  2.27040495e-01  6.33325185e+02]\n",
      "Cost: 1.4212358213302507\n",
      "Iteration 69\n",
      "Parameters: [-7.57961157e-01  1.79937003e+01  7.21606488e+00 -3.36333047e+01\n",
      "  5.72783381e-01  2.57009467e-01  1.56150542e+01  3.82256184e+00\n",
      "  5.50466599e+01  3.37774811e+00  2.26470402e-01  6.33325184e+02]\n",
      "Cost: 1.4177687373768326\n",
      "Iteration 70\n",
      "Parameters: [-7.57960082e-01  1.79937002e+01  7.21606311e+00 -3.36333042e+01\n",
      "  5.72783381e-01  2.57156560e-01  1.56150533e+01  3.82256284e+00\n",
      "  5.50466599e+01  3.37774815e+00  2.25900440e-01  6.33325184e+02]\n",
      "Cost: 1.4143044490179006\n",
      "Iteration 71\n",
      "Parameters: [-7.57959010e-01  1.79937001e+01  7.21606134e+00 -3.36333037e+01\n",
      "  5.72783381e-01  2.57303212e-01  1.56150523e+01  3.82256384e+00\n",
      "  5.50466599e+01  3.37774818e+00  2.25330610e-01  6.33325184e+02]\n",
      "Cost: 1.4108429507472897\n",
      "Iteration 72\n",
      "Parameters: [-7.57957940e-01  1.79937000e+01  7.21605958e+00 -3.36333032e+01\n",
      "  5.72783381e-01  2.57449423e-01  1.56150514e+01  3.82256484e+00\n",
      "  5.50466600e+01  3.37774822e+00  2.24760911e-01  6.33325183e+02]\n",
      "Cost: 1.4073842354562043\n",
      "Iteration 73\n",
      "Parameters: [-7.57956872e-01  1.79936998e+01  7.21605782e+00 -3.36333027e+01\n",
      "  5.72783381e-01  2.57595194e-01  1.56150505e+01  3.82256584e+00\n",
      "  5.50466600e+01  3.37774826e+00  2.24191344e-01  6.33325183e+02]\n",
      "Cost: 1.4039282960243205\n",
      "Iteration 74\n",
      "Parameters: [-7.57955807e-01  1.79936997e+01  7.21605606e+00 -3.36333022e+01\n",
      "  5.72783381e-01  2.57740526e-01  1.56150496e+01  3.82256684e+00\n",
      "  5.50466600e+01  3.37774829e+00  2.23621906e-01  6.33325183e+02]\n",
      "Cost: 1.4004751252664953\n",
      "Iteration 75\n",
      "Parameters: [-7.57954744e-01  1.79936996e+01  7.21605431e+00 -3.36333017e+01\n",
      "  5.72783381e-01  2.57885419e-01  1.56150487e+01  3.82256783e+00\n",
      "  5.50466600e+01  3.37774833e+00  2.23052600e-01  6.33325182e+02]\n",
      "Cost: 1.3970247159444855\n",
      "Iteration 76\n",
      "Parameters: [-7.57953684e-01  1.79936995e+01  7.21605256e+00 -3.36333012e+01\n",
      "  5.72783381e-01  2.58029873e-01  1.56150478e+01  3.82256881e+00\n",
      "  5.50466600e+01  3.37774836e+00  2.22483425e-01  6.33325182e+02]\n",
      "Cost: 1.3935770615622827\n",
      "Iteration 77\n",
      "Parameters: [-7.57952626e-01  1.79936994e+01  7.21605082e+00 -3.36333007e+01\n",
      "  5.72783381e-01  2.58173890e-01  1.56150469e+01  3.82256980e+00\n",
      "  5.50466600e+01  3.37774840e+00  2.21914379e-01  6.33325182e+02]\n",
      "Cost: 1.3901321533428022\n",
      "Iteration 78\n",
      "Parameters: [-7.57951571e-01  1.79936993e+01  7.21604908e+00 -3.36333002e+01\n",
      "  5.72783381e-01  2.58317469e-01  1.56150460e+01  3.82257078e+00\n",
      "  5.50466600e+01  3.37774843e+00  2.21345465e-01  6.33325181e+02]\n",
      "Cost: 1.38668998766248\n",
      "Iteration 79\n",
      "Parameters: [-7.57950518e-01  1.79936992e+01  7.21604734e+00 -3.36332997e+01\n",
      "  5.72783381e-01  2.58460610e-01  1.56150451e+01  3.82257176e+00\n",
      "  5.50466600e+01  3.37774847e+00  2.20776680e-01  6.33325181e+02]\n",
      "Cost: 1.3832505563244117\n",
      "Iteration 80\n",
      "Parameters: [-7.57949468e-01  1.79936991e+01  7.21604561e+00 -3.36332992e+01\n",
      "  5.72783381e-01  2.58603316e-01  1.56150442e+01  3.82257274e+00\n",
      "  5.50466600e+01  3.37774851e+00  2.20208026e-01  6.33325181e+02]\n",
      "Cost: 1.3798138509547488\n",
      "Iteration 81\n",
      "Parameters: [-7.57948420e-01  1.79936990e+01  7.21604389e+00 -3.36332988e+01\n",
      "  5.72783381e-01  2.58745585e-01  1.56150433e+01  3.82257371e+00\n",
      "  5.50466600e+01  3.37774854e+00  2.19639502e-01  6.33325180e+02]\n",
      "Cost: 1.3763798670668144\n",
      "Iteration 82\n",
      "Parameters: [-7.57947374e-01  1.79936989e+01  7.21604216e+00 -3.36332983e+01\n",
      "  5.72783381e-01  2.58887419e-01  1.56150424e+01  3.82257469e+00\n",
      "  5.50466600e+01  3.37774858e+00  2.19071108e-01  6.33325180e+02]\n",
      "Cost: 1.3729485969710424\n",
      "Iteration 83\n",
      "Parameters: [-7.57946331e-01  1.79936988e+01  7.21604044e+00 -3.36332978e+01\n",
      "  5.72783381e-01  2.59028817e-01  1.56150415e+01  3.82257565e+00\n",
      "  5.50466600e+01  3.37774861e+00  2.18502843e-01  6.33325180e+02]\n",
      "Cost: 1.3695200340739115\n",
      "Iteration 84\n",
      "Parameters: [-7.57945290e-01  1.79936987e+01  7.21603873e+00 -3.36332973e+01\n",
      "  5.72783381e-01  2.59169780e-01  1.56150406e+01  3.82257662e+00\n",
      "  5.50466600e+01  3.37774865e+00  2.17934709e-01  6.33325179e+02]\n",
      "Cost: 1.3660941717797765\n",
      "Iteration 85\n",
      "Parameters: [-7.57944252e-01  1.79936986e+01  7.21603702e+00 -3.36332968e+01\n",
      "  5.72783381e-01  2.59310311e-01  1.56150398e+01  3.82257758e+00\n",
      "  5.50466600e+01  3.37774868e+00  2.17366704e-01  6.33325179e+02]\n",
      "Cost: 1.362671001712426\n",
      "Iteration 86\n",
      "Parameters: [-7.57943216e-01  1.79936985e+01  7.21603531e+00 -3.36332963e+01\n",
      "  5.72783381e-01  2.59450407e-01  1.56150389e+01  3.82257854e+00\n",
      "  5.50466600e+01  3.37774872e+00  2.16798829e-01  6.33325179e+02]\n",
      "Cost: 1.359250520848363\n",
      "Iteration 87\n",
      "Parameters: [-7.57942183e-01  1.79936984e+01  7.21603361e+00 -3.36332958e+01\n",
      "  5.72783381e-01  2.59590069e-01  1.56150380e+01  3.82257950e+00\n",
      "  5.50466600e+01  3.37774875e+00  2.16231083e-01  6.33325178e+02]\n",
      "Cost: 1.3558327210300996\n",
      "Iteration 88\n",
      "Parameters: [-7.57941152e-01  1.79936983e+01  7.21603191e+00 -3.36332954e+01\n",
      "  5.72783381e-01  2.59729299e-01  1.56150371e+01  3.82258046e+00\n",
      "  5.50466600e+01  3.37774878e+00  2.15663466e-01  6.33325178e+02]\n",
      "Cost: 1.3524175960939129\n",
      "Iteration 89\n",
      "Parameters: [-7.57940124e-01  1.79936982e+01  7.21603022e+00 -3.36332949e+01\n",
      "  5.72783381e-01  2.59868096e-01  1.56150363e+01  3.82258141e+00\n",
      "  5.50466600e+01  3.37774882e+00  2.15095979e-01  6.33325178e+02]\n",
      "Cost: 1.3490051391339246\n",
      "Iteration 90\n",
      "Parameters: [-7.57939098e-01  1.79936981e+01  7.21602853e+00 -3.36332944e+01\n",
      "  5.72783381e-01  2.60006462e-01  1.56150354e+01  3.82258236e+00\n",
      "  5.50466600e+01  3.37774885e+00  2.14528621e-01  6.33325178e+02]\n",
      "Cost: 1.3455953440929305\n",
      "Iteration 91\n",
      "Parameters: [-7.57938074e-01  1.79936980e+01  7.21602685e+00 -3.36332939e+01\n",
      "  5.72783381e-01  2.60144396e-01  1.56150345e+01  3.82258330e+00\n",
      "  5.50466600e+01  3.37774889e+00  2.13961392e-01  6.33325177e+02]\n",
      "Cost: 1.3421882047987832\n",
      "Iteration 92\n",
      "Parameters: [-7.57937053e-01  1.79936979e+01  7.21602517e+00 -3.36332934e+01\n",
      "  5.72783381e-01  2.60281899e-01  1.56150337e+01  3.82258425e+00\n",
      "  5.50466600e+01  3.37774892e+00  2.13394292e-01  6.33325177e+02]\n",
      "Cost: 1.3387837149659696\n",
      "Iteration 93\n",
      "Parameters: [-7.57936035e-01  1.79936978e+01  7.21602349e+00 -3.36332930e+01\n",
      "  5.72783381e-01  2.60418972e-01  1.56150328e+01  3.82258519e+00\n",
      "  5.50466601e+01  3.37774896e+00  2.12827322e-01  6.33325177e+02]\n",
      "Cost: 1.33538186847592\n",
      "Iteration 94\n",
      "Parameters: [-7.57935016e-01  1.79936977e+01  7.21602182e+00 -3.36332925e+01\n",
      "  5.72783381e-01  2.60555607e-01  1.56150319e+01  3.82258613e+00\n",
      "  5.50466601e+01  3.37774899e+00  2.12260480e-01  6.33325176e+02]\n",
      "Cost: 1.3319826697645978\n",
      "Iteration 95\n",
      "Parameters: [-7.57934002e-01  1.79936976e+01  7.21602015e+00 -3.36332920e+01\n",
      "  5.72783381e-01  2.60691821e-01  1.56150311e+01  3.82258706e+00\n",
      "  5.50466601e+01  3.37774902e+00  2.11693767e-01  6.33325176e+02]\n",
      "Cost: 1.3285860914875616\n",
      "Iteration 96\n",
      "Parameters: [-7.57932991e-01  1.79936975e+01  7.21601849e+00 -3.36332915e+01\n",
      "  5.72783381e-01  2.60827605e-01  1.56150302e+01  3.82258799e+00\n",
      "  5.50466601e+01  3.37774906e+00  2.11127183e-01  6.33325176e+02]\n",
      "Cost: 1.325192138530865\n",
      "Iteration 97\n",
      "Parameters: [-7.57931982e-01  1.79936974e+01  7.21601683e+00 -3.36332911e+01\n",
      "  5.72783381e-01  2.60962961e-01  1.56150294e+01  3.82258892e+00\n",
      "  5.50466601e+01  3.37774909e+00  2.10560727e-01  6.33325175e+02]\n",
      "Cost: 1.3218008045416276\n",
      "Iteration 98\n",
      "Parameters: [-7.57930976e-01  1.79936973e+01  7.21601517e+00 -3.36332906e+01\n",
      "  5.72783381e-01  2.61097889e-01  1.56150285e+01  3.82258985e+00\n",
      "  5.50466601e+01  3.37774912e+00  2.09994400e-01  6.33325175e+02]\n",
      "Cost: 1.3184120841227138\n",
      "Iteration 99\n",
      "Parameters: [-7.57929973e-01  1.79936972e+01  7.21601352e+00 -3.36332901e+01\n",
      "  5.72783381e-01  2.61232389e-01  1.56150277e+01  3.82259078e+00\n",
      "  5.50466601e+01  3.37774916e+00  2.09428202e-01  6.33325175e+02]\n",
      "Cost: 1.315025970679751\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      101\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.548528\n",
      "Convergence reason:              Unknown optimiser specific termination.\n",
      "Cost evaluations at convergence: 101\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.548528\n",
      "Model solve time at convergence: 2.469000\n",
      "Grad solve time at convergence:  19.717000\n"
     ]
    }
   ],
   "source": [
    "bm = ionbench.problems.loewe2016.IKr(sensitivities=True)\n",
    "bm.plotter=False\n",
    "x0 = bm.sample()\n",
    "x_opt = ionbench_gradient_descent(bm, x0)\n",
    "bm.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:09:49.518859Z",
     "start_time": "2024-11-26T12:09:01.790025Z"
    }
   },
   "id": "716af355e70547b2",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our optimiser appears to be working, although not very well. \n",
    "\n",
    "Note the 101 cost function calls that don't do anything in the optimiser.\n",
    "\n",
    "The first thing we will change is to remove the print statements, as these are not needed in ionBench.\n",
    "In particular, finding the cost every iteration is very expensive and doesn't affect the optimisation.\n",
    "\n",
    "Note: ionBench will try to be smart about some model evaluations. It will try to ignore any duplicate model evaluations, so if you call `bm.cost(x)` twice in a row, it will treat it as only one call in the results. It will also try to ignore any calls to `bm.cost(x)` if `bm.grad(x)` was called at the same parameters. Unfortunately, the calls to `bm.cost(x)` and `bm.grad(x)` are different (or at least the call to `bm.cost(x)` comes before the call to `bm.grad(x)`)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec9a4eeca41b4fd9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.548538\n",
      "Convergence reason:              Unknown optimiser specific termination.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.548538\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  25.472000\n"
     ]
    }
   ],
   "source": [
    "def ionbench_gradient_descent(bm, x0, lr=1e-4, tol=1e-6, max_iter=100):\n",
    "    x = copy.deepcopy(x0)  # Initial guess\n",
    "    for i in range(max_iter):\n",
    "        grad = bm.grad(x)  # Find the local gradient\n",
    "        x -= lr * grad  # Take a step in the direction of the -gradient \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            # If locally flat, then terminate\n",
    "            break\n",
    "    return x\n",
    "\n",
    "bm.reset()\n",
    "ionbench_gradient_descent(bm, x0)\n",
    "bm.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:10:30.074501Z",
     "start_time": "2024-11-26T12:09:49.522070Z"
    }
   },
   "id": "7e5b469b7eccbf70",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matching the ionBench style\n",
    "While the above optimiser will work with ionBench, there are a few style choices in the other ionBench optimisers that would be good to match.\n",
    "\n",
    "* The initial guess should be optional, defaulting to None which will implement `bm.sample()`,\n",
    "* The optimiser should call `bm.evaluate()` before returning,\n",
    "* Terminating if the approach has converged (as defined by ionBench),\n",
    "* The optimiser should set the max iteration flag if this occurred during the optimisation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5d701d6c135e8eb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.548538\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.548538\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  23.300000\n",
      "\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.548538\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.548538\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  23.300000\n"
     ]
    }
   ],
   "source": [
    "def ionbench_gradient_descent(bm, x0=None, lr=1e-4, tol=1e-6, max_iter=100):\n",
    "    if x0 is None:\n",
    "        x0 = bm.sample()\n",
    "    x = copy.deepcopy(x0)  # Initial guess\n",
    "    for i in range(max_iter):\n",
    "        grad = bm.grad(x)  # Find the local gradient\n",
    "        x -= lr * grad  # Take a step in the direction of the -gradient \n",
    "        if np.linalg.norm(grad) < tol or bm.is_converged():\n",
    "            break\n",
    "    if i >= max_iter-1:\n",
    "        bm.set_max_iter_flag()\n",
    "    bm.evaluate()\n",
    "    return x\n",
    "\n",
    "bm.reset()\n",
    "ionbench_gradient_descent(bm, x0)\n",
    "bm.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:11:06.043202Z",
     "start_time": "2024-11-26T12:10:30.076731Z"
    }
   },
   "id": "df8c3d51f84d0f69",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we see that ionBench reports that the optimisation terminated due to the maximum number of iterations being reached."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7828b353d5c4853"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Making an approach\n",
    "Now that we have a working optimiser, we can pair it with a modification to create our approach.\n",
    "For our modification, we will use:\n",
    "* Log transforms,\n",
    "* Rate bounds,\n",
    "* and Parameter bounds."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a56c9129d10a74"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.542235\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.542235\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  16.099000\n"
     ]
    }
   ],
   "source": [
    "bm.reset() # Remember bm.reset() also resets modifications, if you don't want this then do bm.reset(fullReset=False)\n",
    "mod = ionbench.modification.Modification(name='new modification', logTransform='on', parameterBounds='on', rateBounds='on', scaleFactors='off')\n",
    "mod.apply(bm)\n",
    "# Apply the transform to x0 (wouldn't be necessary if we did a new `x0=bm.sample()` call here)\n",
    "transformedx0 = bm.input_parameter_space(x0)\n",
    "ionbench_gradient_descent(bm, transformedx0);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:11:33.454325Z",
     "start_time": "2024-11-26T12:11:06.046620Z"
    }
   },
   "id": "b24630e68a37bc54",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "This has improved the convergence of the optimiser (the best cost is lower). Now we can look at testing this approach with `ionBench.multistart` against the same parameters as the original paper."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b102c92b07760fa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.259441\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.259441\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  25.990000\n",
      "\n",
      "[-7.88693775 39.92272363  2.08267479  8.71878688  1.2822942   0.67246299\n",
      "  6.61043947  3.67481362 70.63954153  2.57267894 -2.18475624  5.06939527]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.314284\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.314284\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  25.947000\n",
      "\n",
      "[ -7.79844262  65.17159414  -0.3660141  -46.21168415  -0.5755991\n",
      "   1.53171763  47.47878856   3.57635979  72.43422901   4.48441753\n",
      "  -3.70729395   6.22528885]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.220000\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.220000\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  22.841000\n",
      "\n",
      "[-9.86960550e+00  3.08905232e+01 -3.29808935e-02  5.66930709e+01\n",
      "  1.73449315e+00 -3.92943242e-01 -1.41533238e+01  3.13471261e+00\n",
      "  9.73803737e+00  3.42427776e+00 -5.74228492e+00  5.47614711e+00]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.159581\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.159581\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  21.190000\n",
      "\n",
      "[-7.59553302 28.13207904  3.65286928 25.15123529  0.98688685 -0.29008942\n",
      " 37.81574334 -0.15343469 35.01199896  3.89514922 -4.85950185  3.22572614]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.229317\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.229317\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  14.582000\n",
      "\n",
      "[ -8.96204602  -2.25471606   1.93269421  -4.03501787   3.88278312\n",
      "  -1.83279639 -20.83479163   0.31207477  33.37299878   1.97293663\n",
      "  -3.68222907   3.75780609]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.380300\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.380300\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  14.984000\n",
      "\n",
      "[-9.68477449e+00 -3.26550879e+01  2.32767822e+00 -4.00852710e+01\n",
      "  2.36555143e-01 -6.05259501e-01  5.26191514e+01  1.64776307e-02\n",
      "  5.55533939e+01  1.24901496e+00 -1.33569805e+00  4.79305053e+00]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.209032\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.209032\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  19.377000\n",
      "\n",
      "[ -5.91617022  26.68146214   2.71128033 -51.96466499   0.63366593\n",
      "  -1.74907083 -10.36314479   0.11598993  -6.84202786   2.71438888\n",
      "  -5.53332012   5.8206888 ]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.171904\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.171904\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  21.690000\n",
      "\n",
      "[ -7.80495843 -14.05325305   1.71667571 -45.39434067   1.98363559\n",
      "   1.9768975   -7.67171603   2.64279738 -29.18425535   4.10516345\n",
      "  -4.49611462   3.4754429 ]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.558763\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.558763\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  15.719000\n",
      "\n",
      "[ -7.71468348 -43.48712937   3.12201814 -56.10371668   2.4527284\n",
      "  -1.05895384  42.32323239   3.99992558 -15.14952967   3.45940346\n",
      "  -3.10962684   5.26611802]\n",
      "\n",
      "========================================\n",
      "===      Evaluating Performance      ===\n",
      "========================================\n",
      "\n",
      "Number of cost evaluations:      0\n",
      "Number of grad evaluations:      100\n",
      "Best cost:                       0.791885\n",
      "Convergence reason:              Maximum iterations reached.\n",
      "Cost evaluations at convergence: 0\n",
      "Grad evaluations at convergence: 100\n",
      "Best cost at convergence:        0.791885\n",
      "Model solve time at convergence: 0.000000\n",
      "Grad solve time at convergence:  19.549000\n",
      "\n",
      "[-9.38891473 68.42983899  1.36593754 44.90181678  2.55241852 -0.93551272\n",
      " 51.75562133  1.39560714 60.73258615  3.4751619  -1.77727308  5.81818939]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)  # The set seed for the initial run of ionBench\n",
    "bm.reset(fullReset=False) # Keep the modification\n",
    "ionbench.utils.multistart(opt=ionbench_gradient_descent, bm=bm, initParams=bm.sample(10), filename='');"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-26T12:16:34.849726Z",
     "start_time": "2024-11-26T12:11:33.457489Z"
    }
   },
   "id": "ed58786858a0cbf5",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps\n",
    "Well, our approach didn't do very well. It failed to find the minimum/succeed in any of the runs (since the \"Convergence reason\" was always \"Maximum iterations reached\" and never \"Cost threshold\"). \n",
    "\n",
    "If we wanted to carry on evaluating this approach, we could:\n",
    "* Run for longer until we observed some successes\n",
    "* Evaluate the ERT of the approach using `ionbench.utils.results` (consider checking out the script `reviewOptimiserRuns.py` which does this for all the approaches using outputs from multistart, which can be saved by supplying a filename to multistart)\n",
    "* Evaluate the significance, which can be done with the script `significance.py` (although this requires a lot of runs to be meaningful) which operates of the csv output of `reviewOptimiserRuns.py`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38323bb834960e23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
